#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç”Ÿæˆåˆå¹¶çš„æ•æ„Ÿè¯JSONæ–‡ä»¶
Generate Merged Sensitive Words JSON File

è¯¥è„šæœ¬ä»åˆ†ç±»åçš„æ•æ„Ÿè¯åº“æ–‡ä»¶ç”Ÿæˆä¸€ä¸ªåŒ…å«æ‰€æœ‰è¯æ±‡çš„JSONæ–‡ä»¶ï¼Œ
åŒ…å«é€‚å½“çš„å…ƒæ•°æ®ä¿¡æ¯ã€‚
"""

import os
import json
import logging
from datetime import datetime
from typing import Dict, List, Set

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class MergedJSONGenerator:
    """åˆå¹¶JSONç”Ÿæˆå™¨"""
    
    def __init__(self, classified_dir: str = "classified_vocabulary"):
        """åˆå§‹åŒ–ç”Ÿæˆå™¨
        
        Args:
            classified_dir: åˆ†ç±»è¯åº“ç›®å½•
        """
        self.classified_dir = classified_dir
        self.categories = ['political', 'pornographic', 'violent', 'gambling', 'advertising', 'others']
        
    def load_all_words(self) -> List[str]:
        """åŠ è½½æ‰€æœ‰åˆ†ç±»çš„æ•æ„Ÿè¯
        
        Returns:
            æ‰€æœ‰æ•æ„Ÿè¯çš„åˆ—è¡¨
        """
        all_words = set()
        
        for category in self.categories:
            filename = f"{category}.txt"
            filepath = os.path.join(self.classified_dir, filename)
            
            if os.path.exists(filepath):
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        words = {line.strip() for line in f if line.strip()}
                        all_words.update(words)
                        logger.info(f"åŠ è½½ {category}: {len(words)} ä¸ªè¯æ±‡")
                except Exception as e:
                    logger.warning(f"è¯»å–æ–‡ä»¶ {filepath} æ—¶å‡ºé”™: {e}")
            else:
                logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
        
        # è½¬æ¢ä¸ºæ’åºçš„åˆ—è¡¨
        return sorted(list(all_words))
    
    def generate_merged_json(self, output_path: str = "merged_sensitive_words.json") -> Dict:
        """ç”Ÿæˆåˆå¹¶çš„JSONæ–‡ä»¶
        
        Args:
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            ç”Ÿæˆçš„JSONæ•°æ®
        """
        logger.info("å¼€å§‹ç”Ÿæˆåˆå¹¶çš„æ•æ„Ÿè¯JSONæ–‡ä»¶...")
        
        # åŠ è½½æ‰€æœ‰è¯æ±‡
        all_words = self.load_all_words()
        total_words = len(all_words)
        
        # åˆ›å»ºJSONæ•°æ®ç»“æ„
        merged_data = {
            "lastUpdateDate": datetime.now().strftime("%Y/%m/%d"),
            "totalCount": total_words,
            "description": "åˆå¹¶åçš„æ•æ„Ÿè¯åº“ï¼ŒåŒ…å«æ”¿æ²»ç±»ã€è‰²æƒ…ç±»ã€æš´åŠ›ç±»ã€èµŒåšç±»ã€å¹¿å‘Šç±»ç­‰å¤šç§ç±»å‹çš„æ•æ„Ÿè¯æ±‡",
            "categories": {
                "political": "æ”¿æ²»ç±»",
                "pornographic": "è‰²æƒ…ç±»",
                "violent": "æš´åŠ›ç±»",
                "gambling": "èµŒåšç±»",
                "advertising": "å¹¿å‘Šç±»",
                "others": "å…¶ä»–ç±»"
            },
            "words": all_words
        }
        
        # ä¿å­˜JSONæ–‡ä»¶
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(merged_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… æˆåŠŸç”Ÿæˆåˆå¹¶æ–‡ä»¶: {output_path}")
            logger.info(f"ğŸ“Š æ€»è¯æ±‡æ•°é‡: {total_words}")
            
            # æ˜¾ç¤ºæ–‡ä»¶å¤§å°
            file_size = os.path.getsize(output_path)
            logger.info(f"ğŸ“ æ–‡ä»¶å¤§å°: {file_size:,} å­—èŠ‚ ({file_size/1024:.1f} KB)")
            
            return merged_data
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            raise
    
    def validate_json(self, filepath: str) -> bool:
        """éªŒè¯JSONæ–‡ä»¶çš„æœ‰æ•ˆæ€§
        
        Args:
            filepath: JSONæ–‡ä»¶è·¯å¾„
            
        Returns:
            æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # æ£€æŸ¥å¿…è¦å­—æ®µ
            required_fields = ['lastUpdateDate', 'totalCount', 'words']
            for field in required_fields:
                if field not in data:
                    logger.error(f"âŒ ç¼ºå°‘å¿…è¦å­—æ®µ: {field}")
                    return False
            
            # æ£€æŸ¥è¯æ±‡æ•°é‡æ˜¯å¦åŒ¹é…
            actual_count = len(data['words'])
            declared_count = data['totalCount']
            
            if actual_count != declared_count:
                logger.error(f"âŒ è¯æ±‡æ•°é‡ä¸åŒ¹é…: å£°æ˜ {declared_count}, å®é™… {actual_count}")
                return False
            
            logger.info("âœ… JSONæ–‡ä»¶éªŒè¯é€šè¿‡")
            return True
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSONæ ¼å¼é”™è¯¯: {e}")
            return False
        except Exception as e:
            logger.error(f"âŒ éªŒè¯æ—¶å‡ºé”™: {e}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    generator = MergedJSONGenerator()
    
    # æ£€æŸ¥åˆ†ç±»ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(generator.classified_dir):
        logger.error(f"âŒ åˆ†ç±»ç›®å½•ä¸å­˜åœ¨: {generator.classified_dir}")
        logger.info("è¯·å…ˆè¿è¡Œ classify_vocabulary.py ç”Ÿæˆåˆ†ç±»æ–‡ä»¶")
        return
    
    # ç”Ÿæˆåˆå¹¶JSONæ–‡ä»¶
    try:
        merged_data = generator.generate_merged_json()
        
        # éªŒè¯ç”Ÿæˆçš„æ–‡ä»¶
        if generator.validate_json("merged_sensitive_words.json"):
            logger.info("ğŸ‰ åˆå¹¶JSONæ–‡ä»¶ç”ŸæˆæˆåŠŸï¼")
        else:
            logger.error("âŒ ç”Ÿæˆçš„JSONæ–‡ä»¶éªŒè¯å¤±è´¥")
            
    except Exception as e:
        logger.error(f"âŒ ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºé”™: {e}")

if __name__ == "__main__":
    main()