#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•æ„Ÿè¯æ£€æµ‹ç¤ºä¾‹
Sensitive Word Detection Example

å±•ç¤ºå¦‚ä½•ä½¿ç”¨åˆ†ç±»åçš„æ•æ„Ÿè¯åº“è¿›è¡Œæ–‡æœ¬è¿‡æ»¤
"""

import os
import re
from typing import Dict, List, Set

class SensitiveWordDetector:
    """æ•æ„Ÿè¯æ£€æµ‹å™¨"""
    
    def __init__(self, vocabulary_dir: str = "classified_vocabulary"):
        """åˆå§‹åŒ–æ£€æµ‹å™¨
        
        Args:
            vocabulary_dir: åˆ†ç±»è¯åº“ç›®å½•
        """
        self.vocabulary_dir = vocabulary_dir
        self.word_sets = {}
        self.load_vocabularies()
    
    def load_vocabularies(self):
        """åŠ è½½åˆ†ç±»è¯åº“"""
        categories = ['political', 'pornographic', 'violent', 'gambling', 'advertising', 'others']
        
        for category in categories:
            filename = f"{category}.txt"
            filepath = os.path.join(self.vocabulary_dir, filename)
            
            if os.path.exists(filepath):
                with open(filepath, 'r', encoding='utf-8') as f:
                    words = {line.strip() for line in f if line.strip()}
                    self.word_sets[category] = words
                    print(f"âœ… åŠ è½½ {category}: {len(words)} ä¸ªè¯æ±‡")
            else:
                self.word_sets[category] = set()
                print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
    
    def detect_sensitive_words(self, text: str) -> Dict[str, List[str]]:
        """æ£€æµ‹æ–‡æœ¬ä¸­çš„æ•æ„Ÿè¯
        
        Args:
            text: å¾…æ£€æµ‹çš„æ–‡æœ¬
            
        Returns:
            å­—å…¸ï¼Œé”®ä¸ºç±»åˆ«ï¼Œå€¼ä¸ºæ£€æµ‹åˆ°çš„æ•æ„Ÿè¯åˆ—è¡¨
        """
        results = {}
        
        for category, words in self.word_sets.items():
            found_words = []
            for word in words:
                if word in text:
                    found_words.append(word)
            
            if found_words:
                results[category] = found_words
        
        return results
    
    def filter_text(self, text: str, replacement: str = "*") -> str:
        """è¿‡æ»¤æ–‡æœ¬ä¸­çš„æ•æ„Ÿè¯
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            replacement: æ›¿æ¢å­—ç¬¦
            
        Returns:
            è¿‡æ»¤åçš„æ–‡æœ¬
        """
        filtered_text = text
        
        for category, words in self.word_sets.items():
            for word in words:
                if word in filtered_text:
                    filtered_text = filtered_text.replace(word, replacement * len(word))
        
        return filtered_text
    
    def get_statistics(self) -> Dict[str, int]:
        """è·å–è¯åº“ç»Ÿè®¡ä¿¡æ¯"""
        return {category: len(words) for category, words in self.word_sets.items()}


def main():
    """ç¤ºä¾‹ä¸»å‡½æ•°"""
    print("ğŸš€ æ•æ„Ÿè¯æ£€æµ‹ç¤ºä¾‹")
    print("=" * 50)
    
    # åˆå§‹åŒ–æ£€æµ‹å™¨
    detector = SensitiveWordDetector()
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = detector.get_statistics()
    print(f"\nğŸ“Š è¯åº“ç»Ÿè®¡:")
    for category, count in stats.items():
        print(f"  {category}: {count:,} ä¸ªè¯æ±‡")
    
    # æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "è¿™æ˜¯ä¸€ä¸ªæ­£å¸¸çš„æ–‡æœ¬ï¼Œæ²¡æœ‰æ•æ„Ÿè¯ã€‚",
        "è¿™ä¸ªç½‘ç«™æä¾›å¿«é€ŸåŠè¯æœåŠ¡ï¼Œè¯·è”ç³»QQ123456ã€‚",
        "æ”¿æ²»æ•æ„Ÿå†…å®¹æµ‹è¯•ã€‚",
        "è¿™é‡ŒåŒ…å«ä¸€äº›ä¸å½“å†…å®¹ã€‚",
    ]
    
    print(f"\nğŸ§ª æ£€æµ‹ç¤ºä¾‹:")
    print("-" * 50)
    
    for i, text in enumerate(test_texts, 1):
        print(f"\næµ‹è¯•æ–‡æœ¬ {i}: {text}")
        
        # æ£€æµ‹æ•æ„Ÿè¯
        detected = detector.detect_sensitive_words(text)
        
        if detected:
            print("  æ£€æµ‹åˆ°æ•æ„Ÿè¯:")
            for category, words in detected.items():
                print(f"    {category}: {words}")
        else:
            print("  âœ… æœªæ£€æµ‹åˆ°æ•æ„Ÿè¯")
        
        # è¿‡æ»¤æ–‡æœ¬
        filtered = detector.filter_text(text)
        if filtered != text:
            print(f"  è¿‡æ»¤å: {filtered}")


if __name__ == '__main__':
    main()