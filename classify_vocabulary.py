#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•æ„Ÿè¯åº“åˆå¹¶å’Œåˆ†ç±»è„šæœ¬
Sensitive Lexicon Merging and Classification Script

è¯¥è„šæœ¬ç”¨äºå¤„ç†Vocabularyæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ•æ„Ÿè¯åº“æ–‡ä»¶ï¼Œ
è¿›è¡Œåˆå¹¶ã€å»é‡ã€åˆ†ç±»å’Œæ•´ç†ã€‚
"""

import os
import re
import logging
from datetime import datetime
from collections import defaultdict, Counter
from typing import Dict, List, Set, Tuple

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('classify_vocabulary.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class SensitiveWordClassifier:
    """æ•æ„Ÿè¯åˆ†ç±»å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†ç±»å™¨"""
        self.vocabulary_dir = "Vocabulary"
        self.output_dir = "classified_vocabulary"
        self.categories = {
            'political': 'æ”¿æ²»ç±»',
            'pornographic': 'è‰²æƒ…ç±»', 
            'violent': 'æš´åŠ›ç±»',
            'gambling': 'èµŒåšç±»',
            'advertising': 'å¹¿å‘Šç±»',
            'others': 'å…¶ä»–ç±»'
        }
        
        # åˆ†ç±»å…³é”®è¯æ¨¡å¼
        self.political_patterns = [
            # æ”¿æ²»äººç‰©
            r'(ä¹ |èƒ¡|æ±Ÿ|æ¸©|æ|æœ±|é‚“|æ¯›|å‘¨|åˆ˜|å½­|æ—|é™ˆ|è´º|è‚|å¾|ç½—|å¶).*(å¹³|é”¦|æ³½|å®¶|é¹|é••|å°|æ³½|æ©|å°‘|å¾·|å½ª|ä¼¯|æ¯…|é¾™|è£|å‘|æ¡“|å‰‘)',
            # æ”¿æ²»è¯æ±‡
            r'.*(å…±äº§|ç¤¾ä¼šä¸»ä¹‰|æ°‘ä¸»|è‡ªç”±|ç‹¬ç«‹|åˆ†è£‚|é¢ è¦†|ååŠ¨|æ”¿åºœ|æ”¿æ²»|å…š|ä¸»å¸­|æ€»ç†|ä¹¦è®°)',
            r'.*(æ‰“å€’|æ¨ç¿»|æŠµåˆ¶|åå¯¹).*(ä¸­å›½|å…±äº§|æ”¿åºœ|å…š)',
            r'.*(å°ç‹¬|è—ç‹¬|ç–†ç‹¬|æ¸¯ç‹¬)',
            r'.*(æ³•è½®|è½®åŠŸ|å¤§æ³•)',
            r'(64|å…­å››|å¤©å®‰é—¨|å¹¿åœº)',
            r'.*(æ°‘è¿|å­¦è¿|æ¸¸è¡Œ|ç¤ºå¨|æŠ—è®®)',
        ]
        
        self.pornographic_patterns = [
            r'.*(æ€§|è‰²|æƒ…|æ·«|å¥¸|æ“|å¹²|æ’|è‰|å±Œ|é¸¡|é€¼|å±„|å¦“|å«–|æ˜¥)',
            r'.*(çˆ±æ¶²|æŒ‰æ‘©æ£’|æš´ä¹³|ä¹³æˆ¿|é˜´|ç²¾æ¶²|é«˜æ½®|åšçˆ±|æ€§äº¤)',
            r'.*(Aç‰‡|é»„ç‰‡|è‰²æƒ…|æˆäºº|è£¸|è„±|éœ²)',
        ]
        
        self.violent_patterns = [
            r'.*(æ€|æ­»|è¡€|æš´|æ|ç‚¸|æª|åˆ€|æ¯’|æ‰“|ç |çˆ†|å± |è™)',
            r'.*(è‡ªæ€|ä»–æ€|è°‹æ€|æš´åŠ›|ææ€–|çˆ†ç‚¸|è¢­å‡»)',
            r'.*(ISIS|åŸºåœ°ç»„ç»‡|ææ€–åˆ†å­)',
        ]
        
        self.gambling_patterns = [
            r'.*(èµŒ|åš|å½©ç¥¨|è€è™æœº|ç™¾å®¶ä¹|21ç‚¹|è½®ç›˜|éª°å­)',
            r'.*(æ¾³é—¨|æ‹‰æ–¯ç»´åŠ æ–¯|èµŒåœº|åº„å®¶|ä¸‹æ³¨|æŠ¼æ³¨)',
            r'.*(å…­åˆå½©|æ—¶æ—¶å½©|å¿«ä¸‰|PK10)',
        ]
        
        self.advertising_patterns = [
            r'.*(åŠè¯|åŠç†|ä»£åŠ|åŒ…è¿‡|ä¿è¿‡|å¿«é€Ÿ|ä½ä»·|ä¼˜æƒ |ä¿ƒé”€)',
            r'.*(è´·æ¬¾|å€Ÿé’±|ä¿¡ç”¨å¡|posæœº|åˆ·å¡|å¥—ç°)',
            r'.*(å‘ç¥¨|ç¥¨æ®|è¯ä¹¦|æ–‡å‡­|å­¦å†|èµ„æ ¼è¯)',
            r'.*(å‡è‚¥|ä¸°èƒ¸|ç¾å®¹|æ•´å½¢|è¯å“|ä¿å¥å“)',
            r'.*(å…¼èŒ|æ‹›è˜|ç½‘èµš|åˆ·å•|ç‚¹å‡»|æ¨å¹¿)',
            # ç½‘ç«™å’ŒåŸŸåç›¸å…³
            r'.*\.(com|cn|net|org|info|biz|tv|cc|tk|ml|ga|cf|gq)',
            r'.*www\.|.*http|.*ftp',
            r'.*qq.*\d+.*\..*',  # QQç›¸å…³ç½‘ç«™
            r'\d{3,4}\.\w+\.\w+',  # æ•°å­—å¼€å¤´çš„åŸŸå
        ]

    def read_vocabulary_files(self) -> Dict[str, List[str]]:
        """è¯»å–æ‰€æœ‰è¯æ±‡æ–‡ä»¶"""
        logger.info("å¼€å§‹è¯»å–è¯æ±‡æ–‡ä»¶...")
        file_contents = {}
        
        if not os.path.exists(self.vocabulary_dir):
            logger.error(f"è¯æ±‡ç›®å½• {self.vocabulary_dir} ä¸å­˜åœ¨")
            return {}
            
        for filename in os.listdir(self.vocabulary_dir):
            if filename.endswith('.txt'):
                filepath = os.path.join(self.vocabulary_dir, filename)
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        content = f.read()
                        # å¤„ç†ä¸åŒçš„åˆ†éš”ç¬¦
                        if '\n' in content:
                            words = content.split('\n')
                        else:
                            # å¤„ç†å¯èƒ½çš„ç©ºæ ¼æˆ–å…¶ä»–åˆ†éš”ç¬¦
                            words = re.split(r'[\s,ï¼Œã€]+', content)
                        
                        # æ¸…ç†è¯æ±‡
                        cleaned_words = []
                        for word in words:
                            word = word.strip()
                            if word and not word.startswith('#') and len(word) > 0:
                                cleaned_words.append(word)
                        
                        file_contents[filename] = cleaned_words
                        logger.info(f"è¯»å–æ–‡ä»¶ {filename}: {len(cleaned_words)} ä¸ªè¯æ±‡")
                        
                except Exception as e:
                    logger.error(f"è¯»å–æ–‡ä»¶ {filename} å¤±è´¥: {e}")
                    
        return file_contents

    def classify_word(self, word: str) -> str:
        """å¯¹å•ä¸ªè¯æ±‡è¿›è¡Œåˆ†ç±»"""
        word_lower = word.lower()
        
        # æ£€æŸ¥æ”¿æ²»ç±»
        for pattern in self.political_patterns:
            if re.search(pattern, word, re.IGNORECASE):
                return 'political'
        
        # æ£€æŸ¥è‰²æƒ…ç±»  
        for pattern in self.pornographic_patterns:
            if re.search(pattern, word, re.IGNORECASE):
                return 'pornographic'
        
        # æ£€æŸ¥æš´åŠ›ç±»
        for pattern in self.violent_patterns:
            if re.search(pattern, word, re.IGNORECASE):
                return 'violent'
        
        # æ£€æŸ¥èµŒåšç±»
        for pattern in self.gambling_patterns:
            if re.search(pattern, word, re.IGNORECASE):
                return 'gambling'
        
        # æ£€æŸ¥å¹¿å‘Šç±»
        for pattern in self.advertising_patterns:
            if re.search(pattern, word, re.IGNORECASE):
                return 'advertising'
        
        # é»˜è®¤å½’ç±»ä¸ºå…¶ä»–
        return 'others'

    def classify_by_filename(self, filename: str) -> str:
        """æ ¹æ®æ–‡ä»¶åæ¨æ–­ä¸»è¦ç±»åˆ«"""
        filename_lower = filename.lower()
        
        if 'è‰²æƒ…' in filename or 'porn' in filename_lower:
            return 'pornographic'
        elif 'æš´æ' in filename or 'æš´åŠ›' in filename or 'violent' in filename_lower:
            return 'violent'  
        elif 'ååŠ¨' in filename or 'æ”¿æ²»' in filename or 'political' in filename_lower:
            return 'political'
        elif 'è´ªè…' in filename or 'æ°‘ç”Ÿ' in filename:
            return 'political'  # è´ªè…å’Œæ°‘ç”Ÿé€šå¸¸æ¶‰åŠæ”¿æ²»
        elif 'èµŒ' in filename or 'gambling' in filename_lower:
            return 'gambling'
        elif 'å¹¿å‘Š' in filename or 'ad' in filename_lower:
            return 'advertising'
        else:
            return 'others'

    def process_and_classify(self) -> Tuple[Dict[str, Set[str]], Dict[str, int]]:
        """å¤„ç†å’Œåˆ†ç±»æ‰€æœ‰è¯æ±‡"""
        logger.info("å¼€å§‹å¤„ç†å’Œåˆ†ç±»è¯æ±‡...")
        
        file_contents = self.read_vocabulary_files()
        if not file_contents:
            logger.error("æ²¡æœ‰è¯»å–åˆ°ä»»ä½•è¯æ±‡æ–‡ä»¶")
            return {}, {}
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_words_before = 0
        file_word_counts = {}
        
        # æ”¶é›†æ‰€æœ‰è¯æ±‡å’Œå…¶åˆ†ç±»ï¼Œé¿å…é‡å¤
        word_classifications = {}  # word -> category
        
        for filename, words in file_contents.items():
            total_words_before += len(words)
            file_word_counts[filename] = len(words)
            
            # æ ¹æ®æ–‡ä»¶åæ¨æ–­ä¸»è¦ç±»åˆ«
            main_category = self.classify_by_filename(filename)
            
            logger.info(f"å¤„ç†æ–‡ä»¶ {filename} (æ¨æ–­ç±»åˆ«: {main_category})")
            
            for word in words:
                word = word.strip()
                if len(word) == 0:
                    continue
                
                # å¦‚æœè¯æ±‡å·²ç»åˆ†ç±»è¿‡ï¼Œä½¿ç”¨ä¼˜å…ˆçº§åˆ¤æ–­
                if word in word_classifications:
                    existing_category = word_classifications[word]
                    
                    # å¦‚æœæ–‡ä»¶åæ˜ç¡®æŒ‡å‘æŸä¸ªç±»åˆ«ï¼Œä¼˜å…ˆä½¿ç”¨è¯¥ç±»åˆ«
                    if main_category in ['pornographic', 'violent', 'political'] and main_category != 'others':
                        new_category = main_category
                    else:
                        # å¦åˆ™ä½¿ç”¨æ™ºèƒ½åˆ†ç±»
                        new_category = self.classify_word(word)
                    
                    # ä¼˜å…ˆçº§: political > pornographic > violent > gambling > advertising > others
                    priority = {
                        'political': 6,
                        'pornographic': 5, 
                        'violent': 4,
                        'gambling': 3,
                        'advertising': 2,
                        'others': 1
                    }
                    
                    if priority.get(new_category, 0) > priority.get(existing_category, 0):
                        word_classifications[word] = new_category
                else:
                    # é¦–æ¬¡åˆ†ç±»
                    if main_category in ['pornographic', 'violent', 'political'] and main_category != 'others':
                        category = main_category
                    else:
                        # å¦åˆ™ä½¿ç”¨æ™ºèƒ½åˆ†ç±»
                        category = self.classify_word(word)
                    
                    word_classifications[word] = category
        
        # æŒ‰ç±»åˆ«ç»„ç»‡è¯æ±‡
        classified_words = defaultdict(set)
        for word, category in word_classifications.items():
            classified_words[category].add(word)
        
        # ç»Ÿè®¡å»é‡åçš„è¯æ±‡æ•°é‡
        total_words_after = sum(len(words) for words in classified_words.values())
        
        statistics = {
            'total_before': total_words_before,
            'total_after': total_words_after,
            'duplicates_removed': total_words_before - total_words_after,
            'file_counts': file_word_counts,
            'category_counts': {cat: len(words) for cat, words in classified_words.items()}
        }
        
        logger.info(f"å¤„ç†å®Œæˆ: æ€»è¯æ±‡ {total_words_before} -> {total_words_after}, å»é‡ {statistics['duplicates_removed']} ä¸ª")
        
        return dict(classified_words), statistics

    def create_output_structure(self, classified_words: Dict[str, Set[str]], statistics: Dict[str, int]):
        """åˆ›å»ºè¾“å‡ºæ–‡ä»¶ç»“æ„"""
        logger.info("åˆ›å»ºè¾“å‡ºæ–‡ä»¶ç»“æ„...")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)
            logger.info(f"åˆ›å»ºç›®å½•: {self.output_dir}")
        
        # ç”Ÿæˆåˆ†ç±»æ–‡ä»¶
        for category, words in classified_words.items():
            if not words:
                continue
                
            filename = f"{category}.txt"
            filepath = os.path.join(self.output_dir, filename)
            
            # æŒ‰å­—æ¯é¡ºåºæ’åº
            sorted_words = sorted(list(words))
            
            with open(filepath, 'w', encoding='utf-8') as f:
                for word in sorted_words:
                    f.write(word + '\n')
            
            logger.info(f"ç”Ÿæˆæ–‡ä»¶: {filename} ({len(sorted_words)} ä¸ªè¯æ±‡)")
        
        # ç”ŸæˆREADMEæ–‡æ¡£
        self.create_readme(statistics)
        
        # ç”Ÿæˆç»Ÿè®¡æ–‡ä»¶
        self.create_statistics_file(statistics)

    def create_readme(self, statistics: Dict[str, int]):
        """åˆ›å»ºREADMEæ–‡æ¡£"""
        readme_content = f"""# æ•æ„Ÿè¯åº“åˆ†ç±»ç»“æœ

## æ¦‚è¿°

æœ¬ç›®å½•åŒ…å«ç»è¿‡åˆ†ç±»æ•´ç†çš„æ•æ„Ÿè¯åº“ï¼Œæ‰€æœ‰è¯æ±‡å‡å·²å»é‡å¹¶æŒ‰ç±»åˆ«åˆ†ç±»ã€‚

## æ–‡ä»¶è¯´æ˜

### åˆ†ç±»æ–‡ä»¶
"""

        for category, chinese_name in self.categories.items():
            count = statistics['category_counts'].get(category, 0)
            if count > 0:
                readme_content += f"- **{category}.txt** - {chinese_name} ({count:,} ä¸ªè¯æ±‡)\n"

        readme_content += f"""
### å…¶ä»–æ–‡ä»¶
- **README.md** - æœ¬è¯´æ˜æ–‡æ¡£
- **statistics.txt** - è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯

## ç»Ÿè®¡æ‘˜è¦

- **å¤„ç†å‰æ€»è¯æ±‡æ•°**: {statistics['total_before']:,}
- **å¤„ç†åæ€»è¯æ±‡æ•°**: {statistics['total_after']:,}  
- **å»é™¤é‡å¤è¯æ±‡**: {statistics['duplicates_removed']:,}
- **å»é‡ç‡**: {(statistics['duplicates_removed'] / statistics['total_before'] * 100):.1f}%

## åˆ†ç±»è¯´æ˜

- **æ”¿æ²»ç±» (political.txt)**: åŒ…å«æ”¿æ²»æ•æ„Ÿè¯æ±‡ï¼Œå¦‚æ”¿æ²»äººç‰©ã€æ”¿æ²»äº‹ä»¶ã€æ”¿æ²»æ¦‚å¿µç­‰
- **è‰²æƒ…ç±» (pornographic.txt)**: åŒ…å«è‰²æƒ…ç›¸å…³è¯æ±‡
- **æš´åŠ›ç±» (violent.txt)**: åŒ…å«æš´åŠ›ã€ææ€–ä¸»ä¹‰ç›¸å…³è¯æ±‡
- **èµŒåšç±» (gambling.txt)**: åŒ…å«èµŒåšç›¸å…³è¯æ±‡
- **å¹¿å‘Šç±» (advertising.txt)**: åŒ…å«å¹¿å‘Šã€è¥é”€ã€è¯ˆéª—ç›¸å…³è¯æ±‡
- **å…¶ä»–ç±» (others.txt)**: åŒ…å«å…¶ä»–æ•æ„Ÿè¯æ±‡

## ä½¿ç”¨è¯´æ˜

1. æ ¹æ®éœ€è¦é€‰æ‹©ç›¸åº”çš„åˆ†ç±»æ–‡ä»¶
2. å»ºè®®ç»“åˆå…·ä½“åº”ç”¨åœºæ™¯è°ƒæ•´è¯æ±‡åˆ—è¡¨
3. å®šæœŸæ›´æ–°ä»¥ä¿æŒè¯æ±‡åº“çš„æ—¶æ•ˆæ€§

## ç”Ÿæˆæ—¶é—´

{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}

---

*æ­¤åˆ†ç±»ç»“æœç”±è‡ªåŠ¨åŒ–è„šæœ¬ç”Ÿæˆï¼Œå¦‚æœ‰é—®é¢˜è¯·åŠæ—¶åé¦ˆã€‚*
"""

        readme_path = os.path.join(self.output_dir, 'README.md')
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(readme_content)
        
        logger.info("ç”ŸæˆREADME.mdæ–‡æ¡£")

    def create_statistics_file(self, statistics: Dict[str, int]):
        """åˆ›å»ºç»Ÿè®¡æ–‡ä»¶"""
        stats_content = f"""æ•æ„Ÿè¯åº“å¤„ç†ç»Ÿè®¡æŠ¥å‘Š
======================

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## æ€»ä½“ç»Ÿè®¡

å¤„ç†å‰æ€»è¯æ±‡æ•°: {statistics['total_before']:,}
å¤„ç†åæ€»è¯æ±‡æ•°: {statistics['total_after']:,}
å»é™¤é‡å¤è¯æ±‡: {statistics['duplicates_removed']:,}
å»é‡ç‡: {(statistics['duplicates_removed'] / statistics['total_before'] * 100):.2f}%

## åŸå§‹æ–‡ä»¶ç»Ÿè®¡

"""
        
        for filename, count in statistics['file_counts'].items():
            stats_content += f"{filename}: {count:,} ä¸ªè¯æ±‡\n"

        stats_content += "\n## åˆ†ç±»ç»“æœç»Ÿè®¡\n\n"
        
        for category, chinese_name in self.categories.items():
            count = statistics['category_counts'].get(category, 0)
            percentage = (count / statistics['total_after'] * 100) if statistics['total_after'] > 0 else 0
            stats_content += f"{chinese_name} ({category}.txt): {count:,} ä¸ªè¯æ±‡ ({percentage:.1f}%)\n"

        stats_content += f"\n## å¤„ç†æ—¥å¿—\n\nå¤„ç†å®Œæˆäº: {datetime.now().isoformat()}\n"

        stats_path = os.path.join(self.output_dir, 'statistics.txt')
        with open(stats_path, 'w', encoding='utf-8') as f:
            f.write(stats_content)
        
        logger.info("ç”Ÿæˆstatistics.txtç»Ÿè®¡æ–‡ä»¶")

    def run(self):
        """è¿è¡Œå®Œæ•´çš„åˆ†ç±»æµç¨‹"""
        logger.info("å¼€å§‹æ•æ„Ÿè¯åº“åˆ†ç±»ä»»åŠ¡...")
        start_time = datetime.now()
        
        try:
            # å¤„ç†å’Œåˆ†ç±»
            classified_words, statistics = self.process_and_classify()
            
            if not classified_words:
                logger.error("åˆ†ç±»å¤±è´¥ï¼Œæ²¡æœ‰å¤„ç†åˆ°ä»»ä½•è¯æ±‡")
                return False
            
            # åˆ›å»ºè¾“å‡ºæ–‡ä»¶
            self.create_output_structure(classified_words, statistics)
            
            end_time = datetime.now()
            duration = end_time - start_time
            
            logger.info(f"ä»»åŠ¡å®Œæˆ! è€—æ—¶: {duration}")
            logger.info(f"è¾“å‡ºç›®å½•: {self.output_dir}")
            
            return True
            
        except Exception as e:
            logger.error(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return False


def main():
    """ä¸»å‡½æ•°"""
    classifier = SensitiveWordClassifier()
    success = classifier.run()
    
    if success:
        print("âœ… æ•æ„Ÿè¯åº“åˆ†ç±»ä»»åŠ¡å®Œæˆ!")
        print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {classifier.output_dir}")
    else:
        print("âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—äº†è§£è¯¦æƒ…")
        return 1
    
    return 0


if __name__ == '__main__':
    exit(main())