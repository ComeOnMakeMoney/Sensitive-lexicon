#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•æ„Ÿè¯åº“JSONæ ¼å¼ä¿®å¤è„šæœ¬
Sensitive Words JSON Format Fix Script

è¯¥è„šæœ¬ç”¨äºä¿®å¤ merged_sensitive_words.json æ–‡ä»¶ä¸­çš„æ ¼å¼é—®é¢˜ï¼Œ
ä¸»è¦è§£å†³é€—å·åˆ†éš”è¯æ±‡è¢«é”™è¯¯åœ°åˆå¹¶ä¸ºå•ä¸ªæ¡ç›®çš„é—®é¢˜ã€‚

ä¿®å¤å†…å®¹ï¼š
1. é‡æ–°è¯»å–åŸå§‹çš„æ•æ„Ÿè¯æ±‡æ–‡ä»¶
2. æ­£ç¡®è§£ææ¯ä¸€è¡Œï¼Œç¡®ä¿æ¯ä¸ªè¯æ±‡éƒ½æ˜¯ç‹¬ç«‹çš„JSONæ•°ç»„å…ƒç´ 
3. å¤„ç†åŒ…å«é€—å·åˆ†éš”çš„è¯æ±‡è¡Œ
4. é‡æ–°ç”Ÿæˆæ­£ç¡®æ ¼å¼çš„JSONæ–‡ä»¶
"""

import os
import json
import logging
from datetime import datetime
from typing import List, Set, Dict, Any

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('fix_json_format.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class SensitiveWordsJSONFixer:
    """æ•æ„Ÿè¯åº“JSONæ ¼å¼ä¿®å¤å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ä¿®å¤å™¨"""
        self.vocabulary_dir = "Vocabulary"
        self.output_txt = "merged_sensitive_words.txt"
        self.output_json = "merged_sensitive_words.json"
        self.words_set = set()  # ä½¿ç”¨é›†åˆå»é‡
        
    def read_vocabulary_files(self) -> Set[str]:
        """
        è¯»å–æ‰€æœ‰è¯æ±‡æ–‡ä»¶
        
        Returns:
            Set[str]: å»é‡åçš„æ•æ„Ÿè¯é›†åˆ
        """
        words = set()
        
        if not os.path.exists(self.vocabulary_dir):
            logger.error(f"è¯æ±‡ç›®å½•ä¸å­˜åœ¨: {self.vocabulary_dir}")
            return words
            
        # éå†è¯æ±‡ç›®å½•ä¸­çš„æ‰€æœ‰txtæ–‡ä»¶
        for filename in os.listdir(self.vocabulary_dir):
            if not filename.endswith('.txt'):
                continue
                
            file_path = os.path.join(self.vocabulary_dir, filename)
            logger.info(f"æ­£åœ¨å¤„ç†æ–‡ä»¶: {filename}")
            
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line_num, line in enumerate(f, 1):
                        line = line.strip()
                        
                        # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Šè¡Œ
                        if not line or line.startswith('#'):
                            continue
                            
                        # å¤„ç†é€—å·åˆ†éš”çš„è¯æ±‡
                        if ',' in line:
                            # æŒ‰é€—å·åˆ†å‰²å¹¶å¤„ç†æ¯ä¸ªè¯æ±‡
                            for word in line.split(','):
                                word = word.strip()
                                if word:  # ç¡®ä¿ä¸æ˜¯ç©ºå­—ç¬¦ä¸²
                                    words.add(word)
                                    
                        else:
                            # å•ä¸ªè¯æ±‡
                            words.add(line)
                            
            except Exception as e:
                logger.error(f"è¯»å–æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
                continue
                
        logger.info(f"æ€»å…±è¯»å–åˆ° {len(words)} ä¸ªå”¯ä¸€æ•æ„Ÿè¯")
        return words
        
    def create_merged_txt(self, words: Set[str]) -> bool:
        """
        åˆ›å»ºåˆå¹¶çš„txtæ–‡ä»¶
        
        Args:
            words: æ•æ„Ÿè¯é›†åˆ
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸåˆ›å»º
        """
        try:
            # æ’åºè¯æ±‡ä»¥ä¾¿äºæŸ¥çœ‹å’Œæ¯”è¾ƒ
            sorted_words = sorted(words)
            
            with open(self.output_txt, 'w', encoding='utf-8') as f:
                f.write("# åˆå¹¶æ•æ„Ÿè¯åº“\n")
                f.write(f"# ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"# è¯æ±‡æ€»æ•°: {len(sorted_words)}\n")
                f.write("\n")
                
                for word in sorted_words:
                    f.write(f"{word}\n")
                    
            logger.info(f"æˆåŠŸåˆ›å»ºåˆå¹¶çš„txtæ–‡ä»¶: {self.output_txt}")
            return True
            
        except Exception as e:
            logger.error(f"åˆ›å»ºtxtæ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return False
            
    def create_merged_json(self, words: Set[str]) -> bool:
        """
        åˆ›å»ºåˆå¹¶çš„JSONæ–‡ä»¶
        
        Args:
            words: æ•æ„Ÿè¯é›†åˆ
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸåˆ›å»º
        """
        try:
            # æ’åºè¯æ±‡ä»¥ä¾¿äºæŸ¥çœ‹å’Œæ¯”è¾ƒ
            sorted_words = sorted(words)
            
            # åˆ›å»ºJSONç»“æ„
            json_data = {
                "lastUpdateDate": datetime.now().strftime('%Y/%m/%d'),
                "totalCount": len(sorted_words),
                "description": "ä¸­æ–‡æ•æ„Ÿè¯åº“ - å·²ä¿®å¤æ ¼å¼é—®é¢˜",
                "words": sorted_words
            }
            
            # å†™å…¥JSONæ–‡ä»¶ï¼Œç¡®ä¿UTF-8ç¼–ç å’Œä¸­æ–‡å­—ç¬¦æ­£ç¡®æ˜¾ç¤º
            with open(self.output_json, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)
                
            logger.info(f"æˆåŠŸåˆ›å»ºåˆå¹¶çš„JSONæ–‡ä»¶: {self.output_json}")
            return True
            
        except Exception as e:
            logger.error(f"åˆ›å»ºJSONæ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return False
            
    def generate_statistics(self, words: Set[str]) -> Dict[str, Any]:
        """
        ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        
        Args:
            words: æ•æ„Ÿè¯é›†åˆ
            
        Returns:
            Dict[str, Any]: ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {
            "total_words": len(words),
            "generation_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "source_files": [],
            "word_length_distribution": {}
        }
        
        # ç»Ÿè®¡è¯æ±‡é•¿åº¦åˆ†å¸ƒ
        length_count = {}
        for word in words:
            length = len(word)
            length_count[length] = length_count.get(length, 0) + 1
            
        stats["word_length_distribution"] = dict(sorted(length_count.items()))
        
        # ç»Ÿè®¡æºæ–‡ä»¶
        if os.path.exists(self.vocabulary_dir):
            for filename in os.listdir(self.vocabulary_dir):
                if filename.endswith('.txt'):
                    stats["source_files"].append(filename)
                    
        return stats
        
    def run(self) -> bool:
        """
        è¿è¡Œä¿®å¤ç¨‹åº
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸå®Œæˆä¿®å¤
        """
        logger.info("å¼€å§‹ä¿®å¤æ•æ„Ÿè¯åº“JSONæ ¼å¼...")
        start_time = datetime.now()
        
        try:
            # è¯»å–æ‰€æœ‰è¯æ±‡æ–‡ä»¶
            words = self.read_vocabulary_files()
            
            if not words:
                logger.error("æ²¡æœ‰è¯»å–åˆ°ä»»ä½•è¯æ±‡ï¼Œä¿®å¤å¤±è´¥")
                return False
                
            # åˆ›å»ºåˆå¹¶çš„txtæ–‡ä»¶
            if not self.create_merged_txt(words):
                logger.error("åˆ›å»ºtxtæ–‡ä»¶å¤±è´¥")
                return False
                
            # åˆ›å»ºåˆå¹¶çš„JSONæ–‡ä»¶
            if not self.create_merged_json(words):
                logger.error("åˆ›å»ºJSONæ–‡ä»¶å¤±è´¥")
                return False
                
            # ç”Ÿæˆå¹¶æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            stats = self.generate_statistics(words)
            logger.info("=== ä¿®å¤ç»Ÿè®¡ä¿¡æ¯ ===")
            logger.info(f"æ€»è¯æ±‡æ•°: {stats['total_words']}")
            logger.info(f"æºæ–‡ä»¶æ•°: {len(stats['source_files'])}")
            logger.info(f"ç”Ÿæˆæ—¶é—´: {stats['generation_time']}")
            
            # æ˜¾ç¤ºè¯æ±‡é•¿åº¦åˆ†å¸ƒï¼ˆå‰10ä¸ªï¼‰
            logger.info("è¯æ±‡é•¿åº¦åˆ†å¸ƒï¼ˆå‰10ä¸ªï¼‰:")
            for length, count in list(stats['word_length_distribution'].items())[:10]:
                logger.info(f"  é•¿åº¦ {length}: {count} ä¸ªè¯æ±‡")
                
            end_time = datetime.now()
            duration = end_time - start_time
            logger.info(f"ä¿®å¤å®Œæˆï¼è€—æ—¶: {duration}")
            
            return True
            
        except Exception as e:
            logger.error(f"ä¿®å¤è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}", exc_info=True)
            return False


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("æ•æ„Ÿè¯åº“JSONæ ¼å¼ä¿®å¤å·¥å…·")
    print("Sensitive Words JSON Format Fixer")
    print("=" * 60)
    
    fixer = SensitiveWordsJSONFixer()
    success = fixer.run()
    
    if success:
        print("âœ… JSONæ ¼å¼ä¿®å¤ä»»åŠ¡å®Œæˆ!")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print(f"   - {fixer.output_txt}")
        print(f"   - {fixer.output_json}")
        print("ğŸ“Š ä¿®å¤å†…å®¹:")
        print("   - å¤„ç†é€—å·åˆ†éš”çš„è¯æ±‡è¡Œ")
        print("   - å»é™¤è¯æ±‡å‰åç©ºç™½å­—ç¬¦")
        print("   - ç¡®ä¿JSONæ ¼å¼æ­£ç¡®")
        print("   - æ”¯æŒUTF-8ç¼–ç ä¸­æ–‡å­—ç¬¦")
        print("   - æ›´æ–°è¯æ±‡æ€»æ•°ç»Ÿè®¡")
    else:
        print("âŒ ä¿®å¤ä»»åŠ¡å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—äº†è§£è¯¦æƒ…")
        return 1
        
    return 0


if __name__ == '__main__':
    exit(main())