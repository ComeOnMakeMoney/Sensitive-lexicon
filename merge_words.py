#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•æ„Ÿè¯åº“åˆå¹¶è„šæœ¬
Sensitive Words Merging Script

å°† classified_vocabulary/ ç›®å½•ä¸‹çš„æ‰€æœ‰åˆ†ç±»æ•æ„Ÿè¯åº“æ–‡ä»¶åˆå¹¶æˆä¸€ä¸ªå®Œæ•´çš„æ•æ„Ÿè¯æ±‡æ–‡ä»¶
"""

import os
import sys
from datetime import datetime, timezone
from typing import Dict, List, Tuple

class SensitiveWordsMerger:
    """æ•æ„Ÿè¯åº“åˆå¹¶å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åˆå¹¶å™¨"""
        self.classified_dir = "classified_vocabulary"
        self.output_file = "complete_sensitive_words.txt"
        
        # åˆ†ç±»ä¿¡æ¯ï¼šæ–‡ä»¶åã€ä¸­æ–‡åç§°ã€å¤„ç†çº§åˆ«ã€é¢„æœŸè¯æ±‡æ•°
        self.categories = [
            ("political.txt", "æ”¿æ²»ç±»è¯æ±‡", "BLOCK", 2550),
            ("pornographic.txt", "è‰²æƒ…ç±»è¯æ±‡", "BLOCK", 2819), 
            ("violent.txt", "æš´åŠ›ç±»è¯æ±‡", "BLOCK", 1513),
            ("gambling.txt", "èµŒåšç±»è¯æ±‡", "BLOCK", 133),
            ("advertising.txt", "å¹¿å‘Šç±»è¯æ±‡", "WARN", 19635),
            ("others.txt", "å…¶ä»–ç±»è¯æ±‡", "REVIEW", 16480)
        ]
        
        self.total_expected_words = 43130
    
    def read_vocabulary_file(self, filepath: str) -> List[str]:
        """è¯»å–å•ä¸ªè¯æ±‡æ–‡ä»¶
        
        Args:
            filepath: æ–‡ä»¶è·¯å¾„
            
        Returns:
            è¯æ±‡åˆ—è¡¨
        """
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                words = []
                for line in f:
                    word = line.strip()
                    if word and not word.startswith('#'):
                        words.append(word)
                return words
        except Exception as e:
            print(f"âŒ è¯»å–æ–‡ä»¶ {filepath} å¤±è´¥: {e}")
            return []
    
    def load_all_categories(self) -> Dict[str, List[str]]:
        """åŠ è½½æ‰€æœ‰åˆ†ç±»è¯æ±‡
        
        Returns:
            åˆ†ç±»è¯æ±‡å­—å…¸
        """
        print("ğŸ“š å¼€å§‹åŠ è½½åˆ†ç±»è¯æ±‡æ–‡ä»¶...")
        category_words = {}
        total_words = 0
        
        for filename, chinese_name, level, expected_count in self.categories:
            filepath = os.path.join(self.classified_dir, filename)
            
            if not os.path.exists(filepath):
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
                sys.exit(1)
            
            words = self.read_vocabulary_file(filepath)
            actual_count = len(words)
            
            print(f"âœ… {chinese_name} ({filename}): {actual_count:,} ä¸ªè¯æ±‡ (é¢„æœŸ: {expected_count:,})")
            
            if actual_count != expected_count:
                print(f"âš ï¸ è­¦å‘Š: {filename} è¯æ±‡æ•°é‡ä¸åŒ¹é…")
            
            category_words[filename] = words
            total_words += actual_count
        
        print(f"\nğŸ“Š æ€»è®¡åŠ è½½: {total_words:,} ä¸ªè¯æ±‡")
        
        if total_words != self.total_expected_words:
            print(f"âš ï¸ è­¦å‘Š: æ€»è¯æ±‡æ•° {total_words:,} ä¸ç­‰äºé¢„æœŸçš„ {self.total_expected_words:,}")
        
        return category_words
    
    def generate_file_header(self, total_words: int) -> str:
        """ç”Ÿæˆæ–‡ä»¶å¤´éƒ¨ä¿¡æ¯
        
        Args:
            total_words: æ€»è¯æ±‡æ•°
            
        Returns:
            å¤´éƒ¨ä¿¡æ¯å­—ç¬¦ä¸²
        """
        current_time = datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')
        
        header = f"""# å®Œæ•´æ•æ„Ÿè¯æ±‡åº“ - åŸºäº ComeOnMakeMoney/Sensitive-lexicon
# ç‰ˆæœ¬ï¼šv1.0
# è¯æ±‡æ€»æ•°ï¼š{total_words:,} (å»é‡å)
# ç”Ÿæˆæ—¶é—´ï¼š{current_time}
# æ•°æ®æ¥æºï¼šhttps://github.com/ComeOnMakeMoney/Sensitive-lexicon

"""
        return header
    
    def generate_category_section(self, filename: str, chinese_name: str, 
                                level: str, words: List[str]) -> str:
        """ç”Ÿæˆåˆ†ç±»éƒ¨åˆ†å†…å®¹
        
        Args:
            filename: æ–‡ä»¶å
            chinese_name: ä¸­æ–‡åç§°
            level: å¤„ç†çº§åˆ«
            words: è¯æ±‡åˆ—è¡¨
            
        Returns:
            åˆ†ç±»éƒ¨åˆ†å­—ç¬¦ä¸²
        """
        word_count = len(words)
        
        section = f"""# ============================================
# {chinese_name} [{level}] - {word_count:,} ä¸ª
# ============================================
"""
        
        # æ·»åŠ æ‰€æœ‰è¯æ±‡ï¼Œæ¯è¡Œä¸€ä¸ª
        for word in words:
            section += f"{word}\n"
        
        section += "\n"
        return section
    
    def merge_vocabularies(self) -> bool:
        """åˆå¹¶æ‰€æœ‰è¯æ±‡æ–‡ä»¶
        
        Returns:
            åˆå¹¶æ˜¯å¦æˆåŠŸ
        """
        print("ğŸš€ å¼€å§‹åˆå¹¶æ•æ„Ÿè¯åº“...")
        
        # åŠ è½½æ‰€æœ‰åˆ†ç±»è¯æ±‡
        category_words = self.load_all_categories()
        
        if not category_words:
            print("âŒ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•è¯æ±‡")
            return False
        
        # è®¡ç®—æ€»è¯æ±‡æ•°
        total_words = sum(len(words) for words in category_words.values())
        
        # ç”Ÿæˆåˆå¹¶æ–‡ä»¶
        print(f"ğŸ“ ç”Ÿæˆåˆå¹¶æ–‡ä»¶: {self.output_file}")
        
        try:
            with open(self.output_file, 'w', encoding='utf-8') as f:
                # å†™å…¥æ–‡ä»¶å¤´éƒ¨
                f.write(self.generate_file_header(total_words))
                
                # æŒ‰æŒ‡å®šé¡ºåºå†™å…¥å„åˆ†ç±»
                for filename, chinese_name, level, expected_count in self.categories:
                    if filename in category_words:
                        words = category_words[filename]
                        section = self.generate_category_section(
                            filename, chinese_name, level, words
                        )
                        f.write(section)
                        print(f"âœ… å·²å†™å…¥ {chinese_name}: {len(words):,} ä¸ªè¯æ±‡")
                    else:
                        print(f"âš ï¸ è·³è¿‡ç¼ºå¤±çš„åˆ†ç±»: {filename}")
        
        except Exception as e:
            print(f"âŒ ç”Ÿæˆæ–‡ä»¶å¤±è´¥: {e}")
            return False
        
        print(f"ğŸ‰ åˆå¹¶å®Œæˆ! ç”Ÿæˆæ–‡ä»¶: {self.output_file}")
        print(f"ğŸ“Š æ€»è¯æ±‡æ•°: {total_words:,}")
        
        return True
    
    def generate_statistics(self) -> str:
        """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—ç¬¦ä¸²
        """
        category_words = self.load_all_categories()
        total_words = sum(len(words) for words in category_words.values())
        
        stats = f"""
åˆå¹¶ç»Ÿè®¡ä¿¡æ¯:
=============
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
è¾“å‡ºæ–‡ä»¶: {self.output_file}
æ€»è¯æ±‡æ•°: {total_words:,}

åˆ†ç±»ç»Ÿè®¡:
"""
        
        for filename, chinese_name, level, expected_count in self.categories:
            if filename in category_words:
                actual_count = len(category_words[filename])
                percentage = (actual_count / total_words * 100) if total_words > 0 else 0
                stats += f"- {chinese_name}: {actual_count:,} ä¸ª ({percentage:.1f}%)\n"
        
        return stats


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ”— æ•æ„Ÿè¯åº“åˆå¹¶å·¥å…·")
    print("=" * 60)
    
    merger = SensitiveWordsMerger()
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•
    if not os.path.exists(merger.classified_dir):
        print(f"âŒ åˆ†ç±»è¯æ±‡ç›®å½•ä¸å­˜åœ¨: {merger.classified_dir}")
        return 1
    
    # æ‰§è¡Œåˆå¹¶
    success = merger.merge_vocabularies()
    
    if success:
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        stats = merger.generate_statistics()
        print(stats)
        
        # éªŒè¯è¾“å‡ºæ–‡ä»¶
        if os.path.exists(merger.output_file):
            file_size = os.path.getsize(merger.output_file)
            print(f"âœ… è¾“å‡ºæ–‡ä»¶å¤§å°: {file_size:,} å­—èŠ‚")
        
        print("ğŸ¯ åˆå¹¶ä»»åŠ¡å®Œæˆ!")
        return 0
    else:
        print("âŒ åˆå¹¶ä»»åŠ¡å¤±è´¥!")
        return 1


if __name__ == '__main__':
    sys.exit(main())